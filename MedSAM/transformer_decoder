import numpy as np
import torch
from torch import nn
from torch.nn import init
from collections import OrderedDict
from torch.nn import functional as F
import math 
from segment_anything.build_sam import _build_sam_zm
from segment_anything.modeling.image_encoder import ImageEncoderViT
import os
import argparse
from segment_anything import sam_model_registry
from skimage import io, transform
import torch.nn.functional as F
from functools import partial
from segment_anything.modeling.swin_trans import *
from segment_anything.utils import *
from segment_anything.modeling.blocks_ada.block import *

import cfg_file
args = cfg_file.parse_args()

para_list = ['pos_embed',
              'blocks.2.attn.rel_pos_h',
              'blocks.2.attn.rel_pos_w',
              'blocks.5.attn.rel_pos_h',
              'blocks.5.attn.rel_pos_w',
              'blocks.8.attn.rel_pos_h',
              'blocks.8.attn.rel_pos_w',
              'blocks.11.attn.rel_pos_h',
              'blocks.11.attn.rel_pos_w',
              ]

class BM_encoder(nn.Module):
    def __init__(self,img_size=128,
                 checkpoint="MedSAM/work_dir/MedSAM/medsam_vit_b.pth", 
                 sam_model = _build_sam_zm(args=args),
                 model=ImageEncoderViT(args),
                 ):
        super().__init__()
        self.img_size = img_size
        self.checkpoint = checkpoint
        self.sam ,self.para_list= sam_model
        self.encoder = model
        self.encoder_state_dict={}
        keys,new_para=[],[]
        for k, v in self.sam.state_dict().items():
            if k.startswith('image_encoder.'):
                new_key = k.replace('image_encoder.', '')
                keys.append(new_key)
                self.encoder_state_dict[new_key] = v
        self.encoder.load_state_dict(self.encoder_state_dict,strict=False)

        for k,v in self.para_list.items():
            if k.startswith('image_encoder.'):
                temp = k.replace('image_encoder.', '')
                new_para.append(temp)
                
        for name, param in self.encoder.named_parameters():
            if name in new_para:
                param.requires_grad = False
            else:
                param.requires_grad = True
    def forward(self,x):
        out = self.encoder(x)
        return out



class CAM(nn.Module):
    def __init__(self, channels, r=16, L=32):
        '''
        channels: Bottleneck's planes
        r: reduction ration
        L: Minimum dimension threshold
        '''
        super(CAM, self).__init__()
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.cat_channels = channels * 2
        d = max(int(self.cat_channels/r), L)
        self.fc = nn.Linear(in_features=self.cat_channels, out_features=d, bias=False)
        # fc layer is designed as a dimensionality reduction layer
        self.trans1 = nn.Linear(in_features=d, out_features=round(self.cat_channels/2), bias=False)
        self.trans2 = nn.Linear(in_features=d, out_features=round(self.cat_channels/2), bias=False)
        # the layer's weights are c*(2c/r).
        # parameter matrices
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x1, x2):
        d_A = self.gap(x1)
        d_B = self.gap(x2)
        d_concate = torch.cat((d_A, d_B), dim=1)
        d_concate = d_concate.view(d_concate.size(0), -1)

        g = self.relu(self.fc(d_concate))
        g1 = torch.unsqueeze(self.trans1(g), dim=1)
        g2 = torch.unsqueeze(self.trans2(g), dim=1)
        w = torch.cat((g1, g2), dim=1)
        w = F.softmax(w, dim=1).permute(1, 0, 2)
        w_self = w[0].view(w[0].size(0), w[0].size(1), 1, 1)
        w_other = w[1].view(w[1].size(0), w[1].size(1), 1, 1)

        out1 = w_self * x1 + w_other * x2
        out2 = w_self * x2 + w_other * x1
        return out1, out2

class SAM(nn.Module):
    def __init__(self, h=4, w=4, kernel_size=7, padding=3):
        '''
        h: height
        w: width
        kernel_size, padding: conv layer's params
        '''
        super(SAM, self).__init__()
        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=kernel_size, stride=1, padding=padding, bias=False)
        self.channels = h * w 
        self.trans1 = nn.Linear(in_features=self.channels, out_features=self.channels, bias=False)
        self.trans2 = nn.Linear(in_features=self.channels, out_features=self.channels, bias=False)
        # the layer's weights are hw*hw.
        # parameter matrices
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x1, x2):
        b, c, height, width = x1.size()
        d_A = torch.mean(x1, dim=1, keepdim=True)
        d_B = torch.mean(x2, dim=1, keepdim=True)
        d_concat = torch.cat((d_A, d_B), dim=1)

        g = self.relu(self.conv(d_concat))
        g_vector = g.view(b, -1)
        g1 = torch.unsqueeze(self.trans1(g_vector), dim=1)
        g2 = torch.unsqueeze(self.trans2(g_vector), dim=1)
        w = torch.cat((g1, g2), dim=1)
        w = F.softmax(w, dim=1).permute(1, 0, 2)
        w_self = w[0].view(w[0].size(0), -1, height, width)
        w_other = w[1].view(w[1].size(0), -1, height, width)

        out1 = w_self * x1 + w_other * x2
        out2 = w_self * x2 + w_other * x1
        return out1, out2

class AFA_layer_cam(nn.Module):
    def __init__(self, channels=512):
        super(AFA_layer_cam, self).__init__()
        self.cam = CAM(channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x1, x2):
        x1, x2 = self.cam(x1, x2)
        x1 = self.relu(x1)
        x2 = self.relu(x2)
        return x1, x2

class AFA_layer_sam(nn.Module):
    def __init__(self, h=4, w=4):
        super(AFA_layer_sam, self).__init__()
        self.sam = SAM(h, w)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x1, x2):
        x1, x2 = self.sam(x1, x2)
        x1 = self.relu(x1)
        x2 = self.relu(x2)
        return x1, x2

class ChannelAttention(nn.Module):
    def __init__(self,channel,reduction=16):
        super().__init__()
        self.maxpool=nn.AdaptiveMaxPool2d(1)
        self.avgpool=nn.AdaptiveAvgPool2d(1)
        self.se=nn.Sequential(
            nn.Conv2d(channel,channel//reduction,1,bias=False),
            nn.ReLU(),
            nn.Conv2d(channel//reduction,channel,1,bias=False)
        )
        self.sigmoid=nn.Sigmoid()
    
    def forward(self, x) :
        max_result=self.maxpool(x)
        avg_result=self.avgpool(x)
        max_out=self.se(max_result)
        avg_out=self.se(avg_result)
        output=self.sigmoid(max_out+avg_out)
        return output

class SpatialAttention(nn.Module):
    def __init__(self,kernel_size=7):
        super().__init__()
        self.conv=nn.Conv2d(2,1,kernel_size=kernel_size,padding=kernel_size//2)
        self.sigmoid=nn.Sigmoid()
    
    def forward(self, x) :
        max_result,_=torch.max(x,dim=1,keepdim=True)
        avg_result=torch.mean(x,dim=1,keepdim=True)
        result=torch.cat([max_result,avg_result],1)
        output=self.conv(result)
        output=self.sigmoid(output)
        return output

#The CABM attention module
class CBAMBlock(nn.Module):

    def __init__(self, channel=512,reduction=4,kernel_size=7):
        super().__init__()
        self.ca=ChannelAttention(channel=channel,reduction=reduction)
        self.sa=SpatialAttention(kernel_size=kernel_size)


    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                init.normal_(m.weight, std=0.001)
                if m.bias is not None:
                    init.constant_(m.bias, 0)

    def forward(self, x):
        b, c, _, _ = x.size()
        residual=x
        out=x*self.ca(x)
        t = self.sa(out)
        out=out*t
        return out+residual

# The ECA attention module
class ECAAttention(nn.Module):
    
    def __init__(self, kernel_size=3):
        super().__init__()
        self.gap=nn.AdaptiveAvgPool2d(1)
        self.conv=nn.Conv1d(1,1,kernel_size=kernel_size,padding=(kernel_size-1)//2)
        self.sigmoid=nn.Sigmoid()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                init.normal_(m.weight, std=0.001)
                if m.bias is not None:
                    init.constant_(m.bias, 0)

    def forward(self, x):
        y=self.gap(x) #bs,c,1,1
        y=y.squeeze(-1).permute(0,2,1) #bs,1,c
        y=self.conv(y) #bs,1,c
        y=self.sigmoid(y) #bs,1,c
        y=y.permute(0,2,1).unsqueeze(-1) #bs,c,1,1
        return x*y.expand_as(x)

class ResidualBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, shortcut=None):
        super(ResidualBlock,self).__init__()
        self.left = nn.Sequential(
            nn.Conv2d(in_ch,out_ch,3,stride,padding=1,bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace = True),
            nn.Conv2d(out_ch,out_ch,3,stride=1,padding=1,bias=False),
            nn.BatchNorm2d(out_ch)
            )
        self.right = shortcut
        
    def forward(self,x):
        out = self.left(x)
        residual = x if self.right is None else self.right(x)
        out += residual
        return F.relu(out)
        
# Multi-scale feature extraction
class NewInceptionBlock(nn.Module):
    def __init__(self, in_ch, out_ch1, out_ch2, out_ch3, stride=1):
        super(NewInceptionBlock,self).__init__()
        self.top1 = nn.Sequential(
            nn.Conv3d(in_ch,out_ch1,1,stride,padding=0,bias=False),
            nn.BatchNorm3d(out_ch1),
            nn.ReLU(inplace = True)  
        )
        self.down1 = nn.Sequential(
            nn.Conv3d(out_ch1,out_ch2,3,stride,padding=1,bias=False),
            nn.BatchNorm3d(out_ch2),
            nn.ReLU(inplace = True)  
        )
        self.down2 = nn.Sequential(
            nn.Conv3d(out_ch1,out_ch3,5,stride,padding=2,bias=False),
            nn.BatchNorm3d(out_ch3),
            nn.ReLU(inplace = True)  
        )
    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm3d):
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                init.normal_(m.weight, std=0.001)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
    def forward(self,x):
        out = self.top1(x)
        temp1 = self.down1(out)
        temp2 = self.down2(out)
        result = torch.cat((temp1,temp2),dim=1)
        return result

# Convert the feature map from 2D to 3D    
class TransformBlock(nn.Module):
    def __init__(self, in_ch, k):
        super(TransformBlock,self).__init__()
        self.layer1 = nn.ConvTranspose3d(in_ch,in_ch,(k,1,1),1,0)
        self.layer2 = nn.BatchNorm3d(in_ch)
        self.layer3 = nn.ReLU(inplace = True)
    def forward(self,x):
        result = torch.unsqueeze(x, dim=2)
        result = self.layer1(result)
        result = self.layer2(result)
        result = self.layer3(result)
        return result

# Channel integration
class BasicTranspose3d(nn.Module):
    def __init__(self, in_ch,out_ch, kernel=4, stride=2, padding=1):
        super(BasicTranspose3d,self).__init__()
        self.layer1 = nn.ConvTranspose3d(in_ch,out_ch,kernel,stride,padding)
        self.layer2 = nn.BatchNorm3d(out_ch)
        self.layer3 = nn.ReLU(inplace = True)
    def forward(self,x):
        result = self.layer1(x)
        result = self.layer2(result)
        result = self.layer3(result)
        return result


class transform_layer(nn.Module):
    def __init__(self, in_ch,out_ch, kernel=4, stride=2, padding=1):
        super(BasicTranspose3d,self).__init__()
        self.layer1 = nn.ConvTranspose3d(in_ch,out_ch,kernel,stride,padding)
        self.layer2 = nn.BatchNorm3d(out_ch)
        self.layer3 = nn.ReLU(inplace = True)
    def forward(self,x):
        result = self.layer1(x)
        result = self.layer2(result)
        result = self.layer3(result)
        return result
    
def _initialize_weights(net):
    for m in net.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
            init.kaiming_normal_(m.weight, mode='fan_out')
            if m.bias is not None:
                init.constant_(m.bias, 0)
        elif isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):
            init.kaiming_normal_(m.weight, mode='fan_out')
            if m.bias is not None:
                init.constant_(m.bias, 0)
        elif isinstance(m, nn.InstanceNorm2d) or isinstance(m, nn.InstanceNorm3d) or isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm3d) :
            init.constant_(m.weight, 1)
            init.constant_(m.bias, 0)
            pass
        elif isinstance(m, nn.Linear):
            init.normal_(m.weight, std=0.001)
            if m.bias is not None:
                    init.constant_(m.bias, 0)


# The XctNet network
class XctNet(nn.Module):
    def __init__(self,in_channels, gain=0.02, init_type='standard'):
        super(XctNet,self).__init__()
        self.afa1 = AFA_layer_cam(channels=512)
        self.afa2 = AFA_layer_sam(h=4, w=4)
        self.pre = nn.Sequential(
                nn.Conv2d(in_channels,64,7,stride=2,padding=3,bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace = True)
                )
        
        self.left_cabm1 = CBAMBlock(channel=64,reduction=16,kernel_size=7)
        
        self.left_layer1 = self.make_layer(64,64,3)
        self.left_eca1 = ECAAttention(kernel_size=3)
        self.left_layer2 = self.make_layer(64,128,4,stride=2)
        self.left_eca2 = ECAAttention(kernel_size=3)
        self.left_layer3 = self.make_layer(128,256,6,stride=2)
        self.left_eca3 = ECAAttention(kernel_size=3)
        self.left_layer4 = self.make_layer(256,512,3,stride=2)
        self.left_cabm2 = CBAMBlock(channel=512,reduction=16,kernel_size=3)
        
        self.pre_down = nn.Sequential(
                nn.Conv2d(in_channels,64,7,stride=2,padding=3,bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace = True)
                )
        
        self.left_cabm1_down = CBAMBlock(channel=64,reduction=16,kernel_size=7)
        
        self.left_layer1_down = self.make_layer(64,64,3)
        self.left_eca1_down = ECAAttention(kernel_size=3)
        self.left_layer2_down = self.make_layer(64,128,4,stride=2)
        self.left_eca2_down = ECAAttention(kernel_size=3)
        self.left_layer3_down = self.make_layer(128,256,6,stride=2)
        self.left_eca3_down = ECAAttention(kernel_size=3)
        self.left_layer4_down = self.make_layer(256,512,3,stride=2)
        self.left_cabm2_down = CBAMBlock(channel=512,reduction=16,kernel_size=3)
        
        self.mid_layer1 = TransformBlock(64,32)
        self.mid_layer2 = TransformBlock(128,16)
        self.mid_layer3 = TransformBlock(256,8)
        self.mid_layer4 = TransformBlock(512,4)
        
        self.mid_layers1 = TransformBlock(64,32)
        self.mid_layers2 = TransformBlock(128,16)
        self.mid_layers3 = TransformBlock(256,8)
        self.mid_layers4 = TransformBlock(512,4)
        
        self.right_basic4 = BasicTranspose3d(512,256)
        self.right_layer4 = NewInceptionBlock(256,80,156,100)
        self.right_basic3 = BasicTranspose3d(512,128)
        self.right_layer3 = NewInceptionBlock(128,50,68,60)
        self.right_basic2 = BasicTranspose3d(256,64)
        self.right_layer2 = NewInceptionBlock(64,20,34,30)
        self.right_basic1 = BasicTranspose3d(128,32)
        self.right_layer1 = NewInceptionBlock(32,10,18,14)
        
        self.right_basics4 = BasicTranspose3d(512,256)
        self.right_layers4 = NewInceptionBlock(256,80,156,100)
        self.right_basics3 = BasicTranspose3d(512,128)
        self.right_layers3 = NewInceptionBlock(128,50,68,60)
        self.right_basics2 = BasicTranspose3d(256,64)
        self.right_layers2 = NewInceptionBlock(64,20,34,30)
        self.right_basics1 = BasicTranspose3d(128,32)
        self.right_layers1 = NewInceptionBlock(32,10,18,14)
        
        self.right_basic0 = BasicTranspose3d(32,1)
        self.right_basics0 = BasicTranspose3d(32,2)
        self.conv1_1 = nn.Conv3d(in_channels=2,out_channels=2,kernel_size=1,stride=1,padding=0)
        
        self.urim1 = nn.Softmax(dim=1)
        self.urim2 = nn.Softmax(dim=1)
        _initialize_weights(self)
        
    def make_layer(self,in_ch,out_ch,block_num,stride=1):
        shortcut = nn.Sequential(
                nn.Conv2d(in_ch,out_ch,3,stride=2,padding=1,bias=False),
                nn.BatchNorm2d(out_ch)
                )
        layers = []
        layers.append(ResidualBlock(in_ch,out_ch,stride=2,shortcut=shortcut))
        
        for i in range(1,block_num): 
            layers.append(ResidualBlock(out_ch,out_ch))
        return nn.Sequential(*layers)
        
    def forward(self,x):    
        x_ = self.pre(x)
        x_ = self.left_cabm1(x_)   # n*64*64*64
        
        x_down = self.pre(x)
        x_down = self.left_cabm1_down(x_down)   # n*64*64*64
        
        x1 = self.left_layer1(x_)
        x1 = self.left_eca1(x1) # n*64*32*32
        
        x1_down = self.left_layer1_down(x_down)
        x1_down = self.left_eca1_down(x1_down)
        
        x1_add = self.mid_layer1(x1) #n*64*32*32*32
        x1_sadd = self.mid_layers1(x1_down)
        
        x2 = self.left_layer2(x1)
        x2 = self.left_eca2(x2) # n*128*16*16
        
        x2_down = self.left_layer2_down(x1_down)
        x2_down = self.left_eca2_down(x2_down) # n*128*16*16
        
        x2_add = self.mid_layer2(x2) #n*128*16*16*16
        x2_sadd = self.mid_layers2(x2_down)
        
        x3 = self.left_layer3(x2)
        x3 = self.left_eca3(x3) # n*256*8*8
        
        x3_down = self.left_layer3_down(x2_down)
        x3_down = self.left_eca3_down(x3_down) # n*256*8*8
        
        x3_add = self.mid_layer3(x3) # n*256*8*8*8
        x3_sadd = self.mid_layers3(x3_down)
        
        x4 = self.left_layer4(x3)
        x4 = self.left_cabm2(x4) # n*512*4*4
        
        x4_down = self.left_layer4_down(x3_down)
        x4_down = self.left_cabm2_down(x4_down) # n*512*4*4
        
        x4,x4_down=self.afa1(x4,x4_down)
        x4,x4_down=self.afa2(x4,x4_down)
        
        x4_add = self.mid_layer4(x4) # n*512*4*4*4
        x4_sadd = self.mid_layers4(x4_down)
        
        r_x4 = self.right_basic4(x4_add) # n*256*8*8*8
        r_x4 = self.right_layer4(r_x4) # n*256*8*8*8
        
        s_x4 = self.right_basics4(x4_sadd) 
        s_x4 = self.right_layers4(s_x4)
        
        # reconstruction r_***    segmentation s_***
        r_x34 = torch.cat((r_x4,x3_add),dim=1) # n*512*8*8*8
        r_x34 = self.right_basic3(r_x34) # n*128*16*16*16
        r_x34 = self.right_layer3(r_x34) # n*128*16*16*16
        
        r_x23 = torch.cat((r_x34,x2_add),dim=1) # n*256*16*16*16
        
        s_x34 = torch.cat((s_x4,x3_sadd),dim=1) # n*512*8*8*8
        s_x34 = self.right_basics3(s_x34) # n*128*16*16*16
        s_x34 = self.right_layers3(s_x34) # n*128*16*16*16
        
        s_x23 = torch.cat((s_x34,x2_sadd),dim=1) # n*256*16*16*16
        
        r_x23 = self.right_basic2(r_x23) # n*64*32*32*32
        r_x23 = self.right_layer2(r_x23) # n*64*32*32*32
        
        s_x23 = self.right_basics2(s_x23) # n*64*32*32*32
        s_x23 = self.right_layers2(s_x23) # n*64*32*32*32
        
       
        
        r_x12 = torch.cat((r_x23,x1_add),dim=1) # n*128*32*32*32
        r_x12 = self.right_basic1(r_x12) # n*32*64*64*64
        r_x12 = self.right_layer1(r_x12) # n*32*64*64*64
        
        s_x12 = torch.cat((s_x23,x1_sadd),dim=1) # n*128*32*32*32
        s_x12 = self.right_basics1(s_x12) # n*32*64*64*64
        s_x12 = self.right_layers1(s_x12) # n*32*64*64*64
        
        recon_out = self.right_basic0(r_x12) # n*1*128*128*128
        s_re = self.right_basics0(s_x12)# n*2*128*128*128
        
        score = self.urim1(s_re)
        score_top, _ = score.topk(k=2, dim=1)
        uncertainty = score_top[:, 0] / (score_top[:, 1] + 1e-8)
        uncertainty = torch.exp(1 - uncertainty).unsqueeze(1)
        confidence_map = 1-uncertainty
        temp = self.conv1_1(torch.mul(s_re,confidence_map))
        seg_out = self.urim2(temp)
        
        return recon_out,seg_out
 
class Attention_3D(nn.Module):
    """Multi-head Attention block with relative position embeddings."""

    def __init__(
        self,
        dim: int,
        feature_size:int,
        num_heads: int = 8,
        qkv_bias: bool = True,
        use_rel_pos: bool = False,
        rel_pos_zero_init: bool = True,
        input_size: Optional[Tuple[int, int]] = None,
    ) -> None:
        super().__init__()
        self.feature_size = feature_size
        self.num_heads = num_heads if dim>num_heads else dim
        head_dim = dim // self.num_heads
        self.scale = head_dim**-0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)

        self.use_rel_pos = use_rel_pos
        if self.use_rel_pos:
            assert (
                input_size is not None
            ), "Input size must be provided if using relative positional encoding."
            # initialize relative positional embeddings
            self.rel_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))
            self.rel_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N,C = x.shape
        H,W,K = self.feature_size,self.feature_size,self.feature_size
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        
        q, k, v = qkv.reshape(3, B * self.num_heads, N, -1).unbind(0)

        attn = (q * self.scale) @ k.transpose(-2, -1)

        attn = attn.softmax(dim=-1)
        x = (attn @ v).view(B, self.num_heads, H, W,K,  -1).permute(0, 2, 3, 4,1, 5).reshape(B, N, -1)
        x = self.proj(x)

        return x

class FocusedLinearAttention(nn.Module):
    def __init__(self, dim, num_patches, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1,
                 focusing_factor=3, kernel_size=5):
        super().__init__()
        assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."

        self.dim = dim
        self.num_heads = num_heads
        head_dim = dim // num_heads

        self.q = nn.Linear(dim, dim, bias=qkv_bias)
        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        self.sr_ratio = sr_ratio
        if sr_ratio > 1:
            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)
            self.norm = nn.LayerNorm(dim)

        self.focusing_factor = focusing_factor
        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size,
                             groups=head_dim, padding=kernel_size // 2)
        self.scale = nn.Parameter(torch.zeros(size=(1, 1, dim)))
        self.positional_encoding = nn.Parameter(torch.zeros(size=(1, num_patches // (sr_ratio * sr_ratio), dim)))
        print('Linear Attention sr_ratio{} f{} kernel{}'.
              format(sr_ratio, focusing_factor, kernel_size))

    def forward(self, x, H, W):
        B, N, C = x.shape
        q = self.q(x)

        if self.sr_ratio > 1:
            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)
            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)
            x_ = self.norm(x_)
            kv = self.kv(x_).reshape(B, -1, 2, C).permute(2, 0, 1, 3)
        else:
            kv = self.kv(x).reshape(B, -1, 2, C).permute(2, 0, 1, 3)
        k, v = kv[0], kv[1]

        k = k + self.positional_encoding
        focusing_factor = self.focusing_factor
        kernel_function = nn.ReLU()
        scale = nn.Softplus()(self.scale)
        q = kernel_function(q) + 1e-6
        k = kernel_function(k) + 1e-6
        q = q / scale
        k = k / scale
        q_norm = q.norm(dim=-1, keepdim=True)
        k_norm = k.norm(dim=-1, keepdim=True)
        q = q ** focusing_factor
        k = k ** focusing_factor
        q = (q / q.norm(dim=-1, keepdim=True)) * q_norm
        k = (k / k.norm(dim=-1, keepdim=True)) * k_norm

        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)
        k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)
        v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)

        z = 1 / (q @ k.mean(dim=-2, keepdim=True).transpose(-2, -1) + 1e-6)
        kv = (k.transpose(-2, -1) * (N ** -0.5)) @ (v * (N ** -0.5))
        x = q @ kv * z

        if self.sr_ratio > 1:
            v = nn.functional.interpolate(v.permute(0, 2, 1), size=x.shape[1], mode='linear').permute(0, 2, 1)
        H = W = int(N ** 0.5)
        x = x.transpose(1, 2).reshape(B, N, C)
        v = v.reshape(B * self.num_heads, H, W, -1).permute(0, 3, 1, 2)
        x = x + self.dwc(v).reshape(B, C, N).permute(0, 2, 1)

        x = self.proj(x)
        x = self.proj_drop(x)

        return x


class Trans_Decoder_module(nn.Module):
    
    def __init__(self,
                 dim:int,
                 feature_size:int,
                 num_heads=4,
                 qkv_bias: bool = True,
                 use_rel_pos: bool = False,
                 rel_pos_zero_init: bool = True,
                 input_size: Optional[Tuple[int, int]] = None,
                 window_size:int=0):
        super(Trans_Decoder_module, self).__init__()
        self.norm0 = nn.LayerNorm(dim//8)
        self.norm1 = nn.LayerNorm(dim//4)
        self.attn = Attention_3D(
            dim//4,
            feature_size=feature_size*2,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            use_rel_pos=use_rel_pos,
            rel_pos_zero_init=rel_pos_zero_init,
            input_size=input_size if window_size == 0 else (window_size, window_size),
        )
        self.norm2 = nn.LayerNorm(dim//4)
        self.mlp = MLPBlock(embedding_dim=dim//4,mlp_dim=dim//2,act=nn.GELU)
        self.up_channel = nn.Linear(dim//8,dim//4,bias=False)

        
    def forward(self, x):
        B,C,H,W,K = x.shape
        N = H*W*K
        x=x.view(B,N,C)
        
        chunks = torch.chunk(x,8,dim=-1)
        x= torch.cat(chunks,dim=1)
        
        x = self.norm0(x)
        x = self.up_channel(x)
        
        shortcut =x
        x=self.norm1(x)
        x=self.attn(x)
        x=shortcut+x
        x = x + self.mlp(self.norm2(x))
        x=x.view(B,C//4,H*2,W*2,K*2)
        return x

class T_andCNN_Decoder_module(nn.Module):
    
    def __init__(self,
                 dim:int,
                 feature_size:int,
                 num_heads=8,
                 qkv_bias: bool = True,
                 use_rel_pos: bool = False,
                 rel_pos_zero_init: bool = True,
                 input_size: Optional[Tuple[int, int]] = None,
                 window_size:int=0):
        super(T_andCNN_Decoder_module, self).__init__()
        self.down_conv1 = nn.Conv3d(dim,2*dim,kernel_size=3,padding=1,stride=2)
        self.down_conv2 = nn.Conv3d(2*dim,4*dim,kernel_size=3,padding=1,stride=2)
        self.up_conv1 = nn.ConvTranspose3d(8*dim,2*dim,kernel_size=4, stride=2, padding=1)
        self.up_conv2 = nn.ConvTranspose3d(2*dim,dim//2,kernel_size=4, stride=2, padding=1)
        self.up_conv3 = nn.ConvTranspose3d(dim//2,dim//4,kernel_size=4, stride=2, padding=1)
        self.norm0 = nn.InstanceNorm3d(dim)
        self.norm1 = nn.LayerNorm(dim*8)
        self.attn = Attention_3D(
            dim*8,
            feature_size=feature_size//4,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            use_rel_pos=use_rel_pos,
            rel_pos_zero_init=rel_pos_zero_init,
            input_size=input_size if window_size == 0 else (window_size, window_size),
        )
        self.norm2 = nn.LayerNorm(dim*8)
        self.norm3 = nn.InstanceNorm3d(dim//4)
        self.mlp = MLPBlock(embedding_dim=dim*8,mlp_dim=dim*2,act=nn.GELU)
        self.up_channel = nn.Linear(dim*4,dim*8,bias=False)

        
    def forward(self, x):
        B,C,H,W,K = x.shape
        x = self.norm0(x)
        x=self.down_conv1(x)
        x=self.down_conv2(x)
        
        x=x.view(B,-1,C*4)
        x=self.up_channel(x)
        shortcut =x
        x=self.norm1(x)
        x=self.attn(x)
        x=shortcut+x
        x = x + self.mlp(self.norm2(x))
        
        x=x.view(B,C*8,H//4,W//4,K//4)
        
        x=self.up_conv1(x)
        x=self.up_conv2(x)
        x=self.up_conv3(x)
        x=self.norm3(x)
        return x


class Trans_Decoder(nn.Module):
    def __init__(self,dim,num_modules,feature_map_size):
        super(Trans_Decoder, self).__init__()
        self.modules_list = nn.ModuleList()
        self.channel_scale = 4
        self.space_scale = 2
        for i in range(num_modules):
            module = T_andCNN_Decoder_module(dim=dim//(self.channel_scale**i),
                                          feature_size=feature_map_size*(self.space_scale**i))
            self.modules_list.append(module)
    def forward(self,x):
        for blk in self.modules_list:
            x=blk(x)
        return x

    
class SAM_XCT(nn.Module):
    def __init__(self,in_channels=3, gain=0.02, init_type='standard'):
        super(SAM_XCT,self).__init__()
        self.afa1 = AFA_layer_cam(channels=256)
        self.afa2 = AFA_layer_sam(h=8, w=8)
        
        self.encoder1 = BM_encoder(img_size=128,checkpoint="MedSAM/work_dir/MedSAM/medsam_vit_b.pth")
        self.encoder2 = BM_encoder(img_size=128,checkpoint="MedSAM/work_dir/MedSAM/medsam_vit_b.pth")    
        
        self.decoder_1 = Trans_Decoder(dim=256,num_modules=4,feature_map_size=8)
        self.decoder_2 = Trans_Decoder(dim=256,num_modules=4,feature_map_size=8)
           
        self.mid_layer1 = TransformBlock(256,8)
        self.mid_layer2 = TransformBlock(256,8)
        
        
        self.conv1_1 = nn.Conv3d(in_channels=1,out_channels=2,kernel_size=1,stride=1,padding=0)
        self.conv1_1_2 = nn.Conv3d(in_channels=2,out_channels=2,kernel_size=1,stride=1,padding=0)
        
        self.urim1 = nn.Softmax(dim=1)
        self.urim2 = nn.Softmax(dim=1)
        
        
    def forward(self,x):    
        feature_p1 = self.encoder1(x)
        feature_p2 = self.encoder2(x)
        
        feature_a1,feature_a2 = self.afa1(feature_p1,feature_p2)
        feature_1,feature_2 = self.afa2(feature_a1,feature_a2)
        
        recon_1 = self.mid_layer1(feature_1)
        seg_1 = self.mid_layer2(feature_2)
        
        recon_out = self.decoder_1(recon_1)
        seg = self.decoder_2(seg_1)
        seg = self.conv1_1(seg)
        
        score = self.urim1(seg)
        score_top, _ = score.topk(k=2, dim=1)
        uncertainty = score_top[:, 0] / (score_top[:, 1] + 1e-8)
        uncertainty = torch.exp(1 - uncertainty).unsqueeze(1)
        confidence_map = 1-uncertainty
        temp = self.conv1_1_2(torch.mul(seg,confidence_map))
        seg_out = self.urim2(temp)
        
        return recon_out,seg_out   
